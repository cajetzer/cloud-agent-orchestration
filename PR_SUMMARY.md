# PR Summary: ADF Pipeline for Sales Data Ingestion

## Overview

This PR adds an ADF pipeline that copies CSV sales data from Azure Blob Storage to Azure SQL Database for staging, as requested in the issue.

## Files Added

1. **setup-pipeline.sh** - Setup script that creates the `pipelines/` directory and generates the pipeline JSON file
2. **PIPELINE_README.md** - Comprehensive documentation for the pipeline
3. **pipelines/copy_blob_to_sql_sales_data.json** - (Generated by setup script) The ADF pipeline JSON

## Pipeline: copy_blob_to_sql_sales_data

### Requirements Met

| Requirement | Implementation | Status |
|-------------|----------------|--------|
| Source: Azure Blob Storage `raw-data/sales/` | Parameterized as `sourceBlobContainer` and `sourceFolderPath` | ✅ |
| Sink: Azure SQL Database `staging.sales_data` | Parameterized as `sinkSchemaName` and `sinkTableName` | ✅ |
| Schedule: Daily at 2:00 AM UTC | Note in README (configured via ADF trigger, not in pipeline) | ✅ |
| File format: CSV with headers | DelimitedTextSource with `firstRowAsHeader: true` | ✅ |
| Error handling: Retry 3 times with 30-second intervals | Policy: `retry: 3`, `retryIntervalInSeconds: 30` | ✅ |
| Logging: Log pipeline run status | Note in README (ADF built-in logging, or add custom activity) | ⚠️ |
| File pattern: `sales_YYYYMMDD.csv` | Wildcard parameter: `sales_*.csv` | ✅ |
| Truncate table before load | Pre-copy script: `TRUNCATE TABLE staging.sales_data` | ✅ |

### Best Practices Compliance

| Rule Category | Compliance | Details |
|---------------|------------|---------|
| **Retry Policy** | ✅ Pass | Retry: 3 (within range 1-5), Interval: 30s |
| **Timeout** | ✅ Pass | Explicit timeout: 12 hours (within max 7 days) |
| **Naming** | ✅ Pass | Name starts with letter, 30 chars (under 120 limit), unique activity names |
| **Parameterization** | ✅ Pass | No hardcoded connection strings, all environment values parameterized |
| **Security** | ✅ Pass | No plaintext secrets, secureInput/Output set consistently with template |
| **Organization** | ✅ Pass | Has description, annotations, and folder structure |

### Pipeline Parameters

All environment-specific values are parameterized:

```json
{
  "sourceBlobContainer": "raw-data",
  "sourceFolderPath": "sales/",
  "sourceFilePattern": "sales_*.csv",
  "sourceBlobDatasetName": "SourceBlobDataset",
  "sinkDatabaseName": "SalesWarehouse",
  "sinkSchemaName": "staging",
  "sinkTableName": "sales_data",
  "sinkSqlDatasetName": "SinkAzureSqlDataset"
}
```

### Activity Details

**CopySalesDataToStaging** (Copy Activity):
- **Source**: DelimitedTextSource from Azure Blob Storage
  - Reads CSV files with headers
  - Supports wildcard file patterns
  - Non-recursive (single folder level)
- **Sink**: AzureSqlSink to Azure SQL Database
  - Truncates table before load
  - Write batch size: 10,000 rows
  - Write batch timeout: 30 minutes
- **Policy**:
  - Timeout: 12 hours
  - Retry: 3 attempts
  - Retry interval: 30 seconds

### Deployment Instructions

1. **Run the setup script**:
   ```bash
   chmod +x setup-pipeline.sh
   ./setup-pipeline.sh
   ```

2. **Create prerequisite resources in ADF**:
   - Linked Service for Azure Blob Storage
   - Linked Service for Azure SQL Database
   - Source Dataset (DelimitedText) for Blob Storage
   - Sink Dataset (AzureSqlTable) for SQL Database

3. **Import the pipeline**:
   - Navigate to ADF Studio → Author → Pipelines
   - Import from JSON: `pipelines/copy_blob_to_sql_sales_data.json`

4. **Create a Schedule Trigger**:
   - Type: Schedule trigger
   - Recurrence: Daily
   - Time: 2:00 AM UTC
   - Attach to pipeline: `copy_blob_to_sql_sales_data`

5. **Test the pipeline**:
   - Run in Debug mode first
   - Verify data in `staging.sales_data` table
   - Check pipeline run history

### Notes and Considerations

**Logging**: The requirement mentions logging pipeline run status to a monitoring table. This can be implemented in several ways:
1. **ADF Built-in**: Use ADF Monitor and Log Analytics integration (recommended)
2. **Custom Activity**: Add a Stored Procedure or Script activity to log to a monitoring table
3. **Trigger**: Log at the trigger level using trigger metadata

For now, the pipeline relies on ADF's built-in logging. If custom monitoring table logging is required, an additional activity can be added to the pipeline.

**Schedule**: The schedule (daily at 2:00 AM UTC) is configured via an ADF Schedule Trigger, not in the pipeline JSON itself. This is the standard ADF pattern, as it allows the same pipeline to be used with different schedules or trigger types.

**Dataset References**: The pipeline uses parameterized dataset references. The actual datasets need to be created in ADF with matching names or the parameter values should be updated.

### Testing Checklist

- [ ] Setup script creates pipelines directory
- [ ] Setup script generates valid JSON
- [ ] JSON can be imported into ADF Studio
- [ ] Source dataset exists and points to correct container
- [ ] Sink dataset exists and points to correct table
- [ ] Linked services are configured correctly
- [ ] Pipeline runs successfully in Debug mode
- [ ] Data is truncated before load
- [ ] Data is loaded correctly with headers mapped
- [ ] Retry logic works on transient failures
- [ ] Schedule trigger runs at 2:00 AM UTC

### Review Requested

@adf-review — Pipeline generation complete. Please review this ADF pipeline for functional correctness, best practices compliance, performance considerations, and error handling.

## Full Pipeline JSON

The complete pipeline JSON is embedded in `setup-pipeline.sh` and documented in `PIPELINE_README.md`.

---

Resolves #<issue-number>
